# cpp-microservices-evolution
从单体服务演变到分布式微服务的思路梳理
# 把单体服务器，一步步改成微服务，需要做些什么？  

在之前，我使用C++写了一个通讯服务器的脚手架，包含网络、多线程、连接池等技术。然后基于它，搭建了一个“图书管理系统”出来。这是一个比较典型的**单体应用**，单人开发、测试、运行，还比较方便。  

但是问题来了，如果业务需要扩展呢？比如搞个“10万人在线抢书”的活动，单体服务器肯定直接就挂掉了。那怎样才能让它满足需求呢？

可以把它改成现在流行的微服务架构！

我查了下资料，搞清楚了到底啥是微服务。简单来说，微服务就是：

一种架构风格，它将一个大型的复杂软件应用，拆分成一组小而互相独立的“微”服务。每个服务都运行在自己的进程里，服务之间采用轻量级的通信机制（比如基于HTTP的API）进行沟通。这些服务都围绕着具体的业务功能来构建，并且可以由不同的团队独立开发、部署和扩展。

搞明白定义之后，下面就是我思考的，怎么一步一步从单体改成微服务的思路。

### 第一步：拆解服务

因为面对压力，单体服务无法承受，那么就需要更多的服务一起分担压力。那么最直观的想法就是将服务拆开，比如之前所有功能（用户、图书、借阅）都挤在一个进程里，现在将它们按不同的功能分开，变成三个独立的服务：  

*   **用户服务**  
*   **图书服务**  
*   **借阅服务**

每个服务都是一个独立的进程，可以跑在不同的机器上，用自己独立的数据库表。

拆完之后，问题又来了：外面的人（比如App或者网页）怎么访问这些拆开的服务？请求直接打到每个服务的IP地址上吗？

理论上是可以，但这样会有一堆问题。比如客户端要知道所有服务的地址，不好管理，而且直接把后台服务暴露给外面也不安全。

所以，我们可以搞一个 API网关 (Gateway) 来当统一的入口。

这样，所有请求都先经过网关，网关再根据请求内容，把它转给后面的用户服务、图书服务或者借阅服务。这样一来，外部访问的问题就解决了。

但是，我很快发现，服务拆开、引入网关之后，虽然解决了单体的一些问题，但很多在单体里本来很简单的事情，现在都变成了大问题。 

### 第二步：微服务拆分后的关键问题  

服务拆分后带来了很多的挑战，以下会列出几个最核心的问题，并展示出单体和微服务的区别，以及对应的解决方案。  

#### 挑战一：并发控制 - 从“进程锁”到“分布式锁”  

在业务里，经常要处理并发问题，要用一些技术来避免数据出错。最典型的例子就是：好几个人同时抢同一本书，库存怎么算？

在单体里咋办？
很简单。因为所有请求都在一个进程里，我直接加个锁（比如 C++ 的 std::mutex）就行了。大家排好队，一个一个来改库存，肯定不会出错。

微服务里呢？
这招不行了。因为我可能部署了3个“图书服务”，它们跑在不同的机器上。进程锁管不了别人家的进程。这时候3个服务都以为自己抢到了，都跑去数据库里减库存，那书不就卖超了吗？

如何解决呢？
既然单体里的锁能用，是因为大家都能看见“这把锁”，那在微服务里，我也搞一把所有服务都能看见的“公共锁”，问题不就解决了？
对，所以可以引入分布式锁。我们可以用 Redis 或者 Zookeeper 来实现一个所有服务都能访问到的锁。谁想改库存，就必须先抢到这把锁。谁抢到了，谁才能去操作数据库。这样一来，并发的问题就解决了。

#### 挑战二：数据一致性 - 从“ACID事务”到“最终一致性”  

除了并发，数据一致性也是个大问题。最典型的例子就是：用户借书时，“扣减图书库存”和“创建借阅订单”这两个操作，必须要么都成功，要么都失败。

在单体里咋办？
小事一桩。因为两个操作都在同一个服务里，用的也是同一个数据库，直接用数据库的事务（ACID） 功能把它们捆绑起来就行了。要么打包成功，要么一起失败回滚，数据绝对不会错。

微服务里呢？
老办法又失效了。现在“扣库存”归图书服务管，“创建订单”归借阅服务管，它俩用的是两个独立的数据库。本地事务根本跨不了网线，管不了别的服务。如果在扣完库存后，创建订单的服务突然挂了，那数据就不一致了，这可是严重的Bug。

如何解决呢？
既然一步到位走不通，那就分两步走，保证数据最终是一致的就行。这就是所谓的“最终一致性”。
业界的标准做法是引入消息队列（MQ）。整个流程变成这样：

借阅服务先在自己的数据库里把“借阅订单”创建好。
创建成功后，它不直接去调用图书服务，而是给MQ发一条消息，内容是：“我这边搞定了，该你去扣库存了”。
图书服务一直在监听MQ，收到消息后，就去自己的数据库里执行“扣减库存”的操作。
用这种方法，就算中间某个服务短暂地挂了，只要MQ可靠，消息就不会丢，等服务恢复后还能继续处理。虽然在很短的时间内，数据可能是不一致的（比如订单建好了，库存还没扣），但它最终会达到一致的状态。

#### 挑战三：可用性 - 从“全盘崩溃”到“故障隔离与熔断”  

服务能不能一直正常用，也就是可用性，也是个关键问题。

在单体里咋办？
这是单体最怕的问题。因为所有功能都在一个进程里，任何一个不重要的功能（比如一个很少人用的日志模块）出了Bug，都可能把整个系统搞崩，导致全盘崩溃。

微服务里呢？
微服务也有自己的烦恼。服务之间是互相调用的，比如借阅服务要调用图书服务。如果图书服务因为访问量太大响应变慢了，那所有正在调用它的借阅服务都会被卡住、拖慢。如果情况再严重一点，就会像多米诺骨牌一样，一个传一个，最后导致整个系统“雪崩”。

如何解决呢？
微服务天生就有一个好处：故障隔离。因为服务都拆开了，一个服务挂了，只要跟它没关系的其他服务就还能正常用。比如用户服务挂了，但用户看书、查书的功能不受影响。

但要解决上面说的“雪崩”问题，光靠隔离还不够，还得加上服务熔断。
我们可以给调用方装个“保险丝”，也叫熔断器（比如用 Sentinel 这个库来实现）。它的作用是：

借阅服务在调用图书服务时，熔断器会一直盯着成功率和响应时间。
如果发现调用老是失败或者超时，熔断器就会“拉闸”，主动断开调用。
在接下来的一小段时间里，所有对图书服务的请求都不会真的发出去，而是直接返回一个默认的、友好的结果（比如告诉用户“系统繁忙，请稍后再试”）。
这样一来，就避免了借阅服务被一个有问题的图书服务拖垮，保证了它自己的可用性。

### 第三站：完善架构体系  

解决了上面三个核心挑战后，系统虽然能跑了，但要让它们长期、稳定地跑下去，还得进行完善。

首先是系统内部的通信规范和规则。

服务一多，谁也管不了谁可不行，必须得有规矩。这里面就包含了服务之间怎么“对话”的问题。

规范服务间的通信方式
我们需要规范服务之间通信的方式。服务间的调用，大体上可以分为两类：一类是直接调用，另一类是异步消息。

1. 直接调用：RPC vs HTTP

当一个服务需要立即从另一个服务获取结果时，就需要直接调用。最常用的就是 RPC 和 HTTP。

HTTP API（比如 RESTful 风格）：好处是简单、通用，人都能看懂，调试也方便。缺点是性能相对差一点，因为传输的数据里包含了很多额外信息。
RPC (Remote Procedure Call)：意思是“远程过程调用”。它的性能通常更高，因为数据是二进制的，没那么多废话。用起来感觉就像调用本地函数一样，比如在借阅服务里写 bookService.deductStock(bookId)，感觉不到网络的存在。缺点是不同语言之间搞起来会麻烦一点，需要统一“IDL”（接口定义语言）。
（在高性能、内部服务间调用的场景下，用 RPC 的更多。）

2. 异步消息：消息队列 (MQ)

对于不需要马上得到结果，或者一个操作需要通知多个下游服务的场景，用消息队列就特别合适。

消息队列 (MQ)：就像我们在解决“数据一致性”问题时提到的，借阅服务只需要把“我创建好订单了”这个消息扔进 MQ，就不用管了。下游的图书服务、积分服务、通知服务自己会去 MQ 里面拿消息来处理。这大大降低了服务之间的耦合度，也提高了整个系统的韧性。

我们定义好了通信的方式，但又有一个问题，服务A如何知道服务B在哪呢？

在微服务架构里，服务实例可能会因为扩容、缩容或者故障而动态变化，它们的网络地址不是固定的。所以，服务A不能把服务B的地址写死在配置里。

这时候我们可以使用服务发现机制。

具体做法是引入一个注册中心（比如 Consul, Nacos）。

每个服务实例在启动时，都主动向注册中心“报到”（注册），告诉它自己的IP地址和端口号。
服务实例会定期向注册中心发送“心跳”，证明自己还活着。如果注册中心长时间没收到某个实例的心跳，就会认为它已经下线，并将其从服务列表里剔除。
当服务A需要调用服务B时，它不会直接连接B，而是先去问注册中心：“服务B现在有哪些健康的实例在线？”
注册中心会返回一个服务B的实例地址列表，服务A再从中选择一个（通常会配合负载均衡策略）发起调用。
通过这种方式，就解耦了服务调用方和服务提供方，实现了服务的动态发现。

现在，服务A能找到服务B了，但如果服务B有多个实例，请求到底该发给哪一个呢？

这就需要负载均衡。

当服务A从注册中心拿到服务B的多个实例地址后，它需要一个策略来决定具体调用哪一个。常见的策略有：

轮询：像发牌一样，挨个调用，这次调用第一个，下次调用第二个。
随机：随便选一个。
加权轮询：可以给性能更好的机器分配更高的权重，让它接收更多的请求。
负载均衡保证了请求能均匀地分发到各个服务实例上，避免了单个实例压力过大的问题。

常见的负载均衡器：

服务端负载均衡：在请求到达后端服务之前进行转发，比如 Nginx、F5。
客户端负载均衡：调用方（客户端）自己决定调用哪个实例，比如 Spring Cloud LoadBalancer、Dubbo 内置的负载均衡策略。
解决了服务发现和负载均衡，还有一个常见的问题：每个服务都有自己的配置，比如数据库地址、线程池大小等，这些配置不能写死在代码里，否则每次修改都得重新部署一次服务，太麻烦了。

为了解决这个问题，我们需要引入配置中心。

我们可以把所有服务的配置都集中存放在配置中心里（比如 Apollo, Nacos）。

服务启动时，会从配置中心拉取属于自己的那份配置。
配置中心还支持在不重启服务的情况下，动态地修改配置。当管理员在配置中心更新了某个配置项后，配置中心会通知对应的服务实例来获取最新的配置，服务在运行时就能应用新的配置值。
这样就实现了配置和代码的分离，大大提升了运维效率。

#### 2. 增强可观测性 

我们通过服务治理制定了服务间的规则，让它们可以有序地协同工作。但系统上线后，我们如何知道它跑得好不好？出了问题又该如何快速定位？

这就需要建立系统的可观测性。

如果没有有效的可观测性手段，那么当线上出现问题时，我们就如同“睁眼瞎”，无法定位问题根源。可观测性主要由三部分构成：日志、指标和追踪。

1. 日志: 记录“发生了什么事”

当服务出现错误时，最直接的排查手段就是看日志。但在微服务架构中，一个用户的请求可能会流经多个服务，每个服务都会产生自己的日志，这些日志散落在各处，排查起来非常困难。

因此，我们需要集中式日志系统。

我们可以将所有服务实例的日志都采集并发送到一个统一的地方进行存储和检索，比如使用 ELK (Elasticsearch, Logstash, Kibana) 技术栈或者 Loki。

为了能将分散在不同服务中的日志串联起来，我们需要引入 Trace ID。当一个请求首次进入系统时，就为它生成一个全局唯一的 Trace ID，并让这个ID在后续的所有服务调用中一直传递下去。这样，当需要排查问题时，只要用这个 Trace ID 在日志系统里一搜，就能看到这个请求从头到尾的完整日志记录。

2. 指标: 衡量“系统运行得怎么样”

日志告诉我们发生了什么，而指标则告诉我们系统的宏观状态是否健康。我们需要持续监控关键的性能指标。

每个服务都会暴露一系列反映其健康状况的指标，如 QPS（每秒请求数）、响应延迟、CPU和内存使用率、错误率等。我们可以使用 Prometheus 来周期性地抓取和存储这些指标数据。

光有数据还不够，还需要将它们以图表的形式直观地展示出来。Grafana 就是一个非常流行的工具，它可以连接到 Prometheus 数据源，将枯燥的指标数据绘制成直观的监控大盘，让系统的健康状况一目了然。

3. 追踪: 分析“调用链路和瓶颈在哪”

当发现某个请求的响应时间特别长时，我们怎么知道到底是慢在了哪个环节？这就需要分布式追踪。

利用我们之前提到的 Trace ID，追踪系统可以将一个请求经过的所有服务、以及在每个服务内部的调用细节，串联成一条完整的调用链，并以“火焰图”或“甘特图”的形式进行可视化。

常用的追踪系统有 Jaeger 和 SkyWalking。通过它们生成的调用链图，我们可以清晰地看到每个环节的耗时，从而快速定位性能瓶颈。

